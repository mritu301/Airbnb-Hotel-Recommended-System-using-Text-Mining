---
title: "Matrix_Factorization"
author: "Mritunjay And Sunil"
date: "15/10/2019"
output:
  html_document: default
  word_document: default
  pdf_document: default
---
  
# Recommended System Techniques on Airbnb (Amsterdam Hotel Recommendation)
# 1. Alternating Least Squares for Recommendation System
### Load all the User build functions
```{r}
# Make recommendations for the target user using User-based CF
getrecommendations_UU <- function(targetuser, users, topN=5, simfun=pearsonsim) {
  sims = apply(users,1,function(user) simfun(user,targetuser)) 
  sims = sims[!is.na(sims) & sims >=0]
  wavrats = apply(users[names(sims),is.na(targetuser), drop=FALSE],2,function(rats) weighted.mean(rats, sims, na.rm=TRUE))
  s = sort(wavrats[!is.na(wavrats)], decreasing = TRUE)
  if (topN == FALSE) s else s[1:min(topN,length(s))] # get topN items
}
#getrecommendations_UU = cmpfun(getrecommendations_UU)

# get recommedations for the target user using Item-based CF
getrecommendations_II <- function(targetuser, itemsims, topN=5) {
  targetuser = targetuser[colnames(itemsims)] # ensure the item order is the same as simmatrix
  seenitems  = !is.na(targetuser)
  unseenitems = is.na(targetuser)
  seenrats = targetuser[seenitems]
  preds = apply(itemsims[unseenitems,seenitems, drop=FALSE], 1, function(simrow) my.weighted.mean(seenrats, simrow))
  sp = sort(preds[!is.na(preds)] , decreasing = TRUE)
  sp[1:min(topN,length(sp))]  # get topN items
}
#getrecommendations_II = cmpfun(getrecommendations_II)

# compute the item-item similarity matrix (the matrix is symmetric so can compute half & then copy)
# (setting dir=1 generates the user similarity matrix)
getitemsimsmatrix = function(users, simfun=cosinesim, dir=2) {
  rw <<- 1; 
  itemsims = apply(users, dir, function(itemA) {
    rw <<- rw + 1 ; cl <<- 1; 
    apply(users,dir,function(itemB) {cl<<-cl+1; if (cl<rw) NA else if (cl==rw) NA else simfun(itemA,itemB)})
  })
  m = forceSymmetric(itemsims,uplo="L") # copy lower half to upper half
  as.matrix(m)
}
#getitemsimsmatrix = cmpfun(getitemsimsmatrix)

# similarity functions
euclidsim = function(x,y) { z=(y-x)^2; sz=sqrt(sum(z,na.rm=TRUE));
                            if (sz!=0) 1/(1+sz) else if (length(which(!is.na(z)))==0) NA else 1/(1+sz)}

euclidsimF= function(x,y) { z=(y-x)^2; sz=sum(z,na.rm=TRUE);
                            if (sz!=0) 1/(1+sz) else if (length(which(!is.na(z)))==0) NA else 1/(1+sz)} 

cosinesim = function(x,y) { xy = x*y; sum(xy, na.rm=TRUE)/(sqrt(sum(x[!is.na(xy)]^2)*sum(y[!is.na(xy)]^2)))}

pearsonsim= function(x,y) { suppressWarnings(cor(unlist(x),unlist(y),use="pairwise.complete.obs")) }

mypearsim = function(x,y) { xy = x*y; x=x[!is.na(xy)]; y=y[!is.na(xy)]; 
                            mx=mean(x); my=mean(y);
                            sum((x-mx)*(y-my))/(sqrt(sum((x-mx)^2)*sum((y-my)^2)))}

pearsonRM = function(x,y) { mx=mean(x,na.rm=TRUE);my=mean(y,na.rm=TRUE);
                            xy=x*y;x=x[!is.na(xy)]; y=y[!is.na(xy)]
                            sum((x-mx)*(y-my))/(sqrt(sum((x-mx)^2)*sum((y-my)^2)))}

jacardsim = function(x,y) { validx= !is.na(x); validy= !is.na(y); 
                            sum(as.integer(validx&validy))/sum(as.integer(validx|validy))}

###############################################################################
# For testing, we split the data by user, so test users are not in the trainset
# This is clean but does not test the situation where partial information 
# is known about a user (as may be the case in User-based scenario).
# For item-based having partial info will make very little difference (since simmatrix is precomputed)
###############################################################################

# make predicted ratings for a sample of items for each test user
# if trainusers is defined then do User-based CF else do Item-based CF
# Note: if Item-based CF is to be performed them the itemsimilarity matrix (itemsims) must be defined
predictCF = function(testusers, trainusers=NULL, itemsims=NULL, numtestitems=10, random=FALSE, simfun=cosinesim) {
  preds = sapply(1:nrow(testusers),function(i) {
    cat(".")
    predictuser(testusers[i,],trainusers=trainusers,itemsims=itemsims,numtestitems=numtestitems,random=random,simfun=simfun)})
  colnames(preds) = rownames(testusers)
  preds
}

predictuser <- function(testuser, trainusers=NULL, itemsims=NULL, numtestitems=10, random=FALSE, simfun=cosinesim) {
  seenitemnames   = names(testuser)[!is.na(testuser)]
  if (random) testitemnames = sample(seenitemnames,min(numtestitems,length(seenitemnames))) # test a random N items
  else testitemnames = seenitemnames[1:min(numtestitems,length(seenitemnames))] # test first N items
  preds = list()
  for (testitemname in testitemnames) {
    truerating = testuser[testitemname] 
    testuser[testitemname] = NA
    if (!is.null(trainusers)) {
      # do user-based CF
      usersims = apply(trainusers,1,function(trainuser) simfun(trainuser,testuser))
      usersims = usersims[!is.na(usersims) & usersims >=0]
      predictedrating = my.weighted.mean(trainusers[names(usersims),testitemname], usersims)
    }
    else {
      # do item-based CF
      predictedrating = my.weighted.mean(testuser[seenitemnames], itemsims[seenitemnames,testitemname])
    }
    testuser[testitemname] = truerating # restore the actual rating
    preds = c(preds,predictedrating,truerating)
  }
  preds = unname(preds)
  m = as.matrix(preds)
  if (length(m) < numtestitems*2) for (i in (length(m)+1):(numtestitems*2)) { m = rbind(m,NA)}
  return(m)
}
#predictuser= cmpfun(predictuser)

# a weighted mean that handles NA's in both arguments (ratings and similarities)
my.weighted.mean = function(x,y) {
    xy = x*y; 
    z = sum(abs(y[!is.na(xy)]))
    if (z == 0) as.numeric(NA) else sum(xy,na.rm=TRUE)/z 
}
#my.weighted.mean = cmpfun(my.weighted.mean)

# computes average, mean absolute error
# each row contains prediction, actual, prediction, actual etc, hence errors are just the diff between consecutive cells
avgMAE = function(preds) {
  plist = unlist(preds)
  errors = sapply(1:(length(plist)/2),function(i) abs(plist[i*2-1]-plist[i*2]))
  errors = errors[errors != Inf]
  mean(errors,na.rm=TRUE)
}

showCM = function(preds, like) {
  plist = unlist(preds)
  cnts = sapply(1:(length(plist)/2), function(i) {
    pred = plist[i*2-1] ; actual = plist[i*2]
    if (!is.na(pred) & !is.nan(actual)) {
      if (pred>=like) {if(actual>=like) c(1,0,0,0) else c(0,1,0,0)}
      else if(actual<like) c(0,0,1,0) else c(0,0,0,1) 
    } else c(0,0,0,0)
  })
  s = rowSums(cnts)   #returns cnts for: TP, FP, TN, FN

  cat(sprintf("TN=%5d FP=%5d\n",s[3],s[2]))
  cat(sprintf("FN=%5d TP=%5d  (total=%d)\n",s[4],s[1], sum(s)))
  cat(sprintf("accuracy  = %0.1f%%\n",(s[1]+s[3])*100/sum(s)))
  cat(sprintf("precision = %3.1f%%\n",s[1]*100/(s[1]+s[2])))
  cat(sprintf("recall    = %3.1f%%\n",s[1]*100/(s[1]+s[4])))
}

#######################
# miscellaneous aids
#######################

maketraintest = function(users,numtestusers) {
  testnames  = sample(rownames(users), min(numtestusers,nrow(users))) # identify N users randomly for testing
  trainnames = setdiff(rownames(users),testnames) # take remaining users for training
  trainusers <<- users[trainnames,]
  testusers  <<- users[testnames,]
  list(trainusers,testusers)
}

# extract only prediction or only actual ratings from the output of predictCF()
listpreds= function(results) {unlist(results)[c(TRUE,FALSE)]}
listrats = function(results) {unlist(results)[c(FALSE,TRUE)]}
validcnt = function(x) length(which(is.finite(x)))

# How sparse is the data in a data frame? Compute % of non-blank entries
fillrate = function(df) {cat((length(which(!is.na(df)))*100)/(nrow(df)*ncol(df)),"%")}
#fillrate = cmpfun(fillrate)

# same as above but also works on vectors
fillratev = function(df) {t=unlist(df); cat((length(which(!is.na(t)))*100)/length(t),"%")}
#fillratev = cmpfun(fillratev)

# how many values are > 0? Compute % of entries > 0
fillrateG = function(df,thresh) {t=unlist(df); cat((length(which(!is.na(t) & t > thresh))*100)/length(t),"%")}
fillrateL = function(df,thresh) {t=unlist(df); cat((length(which(!is.na(t) & t < thresh))*100)/length(t),"%")}
fillrateE = function(df,thresh) {t=unlist(df); cat((length(which(!is.na(t) & t == thresh))*100)/length(t),"%")}
```

### Load all the relevant libraries and Get the working directory and Load the Amsterdam Hotel Airbn data set
```{r}
pacman::p_load(tidyverse, purrr, stringr, data.table, modelr, readxl,caret, corrplot, broom, ggpubr, tm, proxy, MASS,relaimpo, car,interplot, caTools, mice, gbm, reshape2, compiler, recommenderlab, Matrix, knitr,tidyr, dplyr, animation, wordnet, RColorBrewer, wordcloud, SnowballC, topicmodels, ggplot2, cluster, fpc, recosystem, dtplyr,softImpute)
getwd()
airbnb = read.csv("airbnb.csv", header=TRUE, sep=",") # transaction format!
names(airbnb) = c(colnames(airbnb))
head(airbnb,1)
```

### Structure of Datasets
```{r}
str(airbnb)
```
### Removing all those users corresponding to missing ratings and
### Extract only the explicit ratings and visualize the histogram of Ratings
```{r}
airbnbCF = airbnb[,c("Hotel_Id","User_Id", "Ratings")]
sapply(airbnbCF, function(x){sum(is.na(x))})
airbnbCF$Ratings[is.na(airbnbCF$Ratings)] = 0
airbnbCF = airbnbCF[airbnbCF$Ratings > 0,]
sapply(airbnbCF, function(x){sum(is.na(x))})
hist(airbnbCF$Ratings)
```



### Eliminate users with too few ratings and Consider Activer users who had rated hotels more than and equal to 10 hotels
```{r}
cnts = aggregate(Hotel_Id ~ User_Id, data = airbnbCF, FUN = length)
colnames(cnts) = c("user","numitems")
activeusers = cnts$user[cnts$numitems >= 10] ; length(activeusers)
evCF = airbnbCF[airbnbCF$User_Id %in% activeusers,]
dim(evCF)
```



### Eliminate Hotels with too few ratings and Consider Active Hotels who had been rated more than and equal to 10 users
```{r}
cnts = aggregate(User_Id ~ Hotel_Id, data = airbnbCF, FUN=length)
colnames(cnts) = c("item","numusers")
popularhotels = cnts$item[cnts$numusers >= 10] ; length(popularhotels)
ev = evCF[evCF$Hotel_Id %in% popularhotels,]
dim(ev)
str(ev)
```



### Remove duplicate records from the datasets
```{r}
ev_Final = ev %>% distinct(User_Id,Hotel_Id,.keep_all = TRUE)
dim(ev_Final)
str(ev_Final)
```

#####################################
# (2) using the softImpute library
##################################### 

### reread the data ensuring users and items are read as factors
```{r}
events = ev_Final[,c(2,1,3)]
ctypes = c("factor","factor","numeric")
colnames(events) = c("user","item","rating")
events$user= factor(events$user)
events$item= factor(events$item)
str(events)
```

### Create a wide format of dataset
```{r}
users = acast(events, user ~ item, value.var = "rating")
#colnames(users) = sort(unique(events$item))
#rownames(users) = sort(unique(events$user))
users[1:10,1:15]
```

### split the events using the same split (train_ind & test_ind) as used earlier
```{r}
set.seed(123)
smp_size <- floor(0.8 * nrow(events))
train_indexes <- sample(1: nrow(events), size = smp_size)
trainevents <- events[train_indexes, ]; dim(trainevents)
testevents  <- events[-train_indexes, ]; dim(testevents)
write.csv(trainevents, "trainevents.csv")
write.csv(testevents, "testevents.csv")
```

### make a copy and then blank out the test events (ie set test ratings for the test (user,item) pairs to NA)
```{r}
trainusers = users
cat("Fill rate whole wide matrix : "); 
fillrate(trainusers)
cat("\n")
cat("Fill rate Testset matrix : "); 
x = apply(testevents,1,function(row) trainusers[row[1],row[2]] <<- NA) # row[1] ~ user, row[2] ~ item
fillrate(trainusers)
```

### factorize into U * D * V using 30 latent features
```{r}
trainusers=as(trainusers,"Incomplete") # coerce into correct matrix format with missing entries
```

### do one of the below
```{r}
fit1=softImpute(trainusers, rank.max=30, type="als") # als is the default
fit2=softImpute(trainusers, rank.max=30, type="svd") # for comparison
```

### take a look at the factorised matrixes
```{r}
dim(fit1$u) ; fit1$u[1:10,1:5] # the user latent features
dim(fit1$v) ; fit1$v[1:10,1:5] # the item latent features
length(fit1$d); head(fit1$d)   # the singular values
```

### make predictions for all of the empty (user,item) pairs (the test pairs + those missing in orginal dataset)
```{r}
trainuserscompleted1 = complete(trainusers, fit1)
trainuserscompleted2 = complete(trainusers, fit2)
```

### compute the MAE for the predictions made for the test events fir model 1 - fit1 (Using ALS)
```{r}
rownames(trainuserscompleted1) = rownames(users) # copy across the user names
colnames(trainuserscompleted1) = colnames(users) # copy across the item names
# Output recommendation using ALS.
trainuserscompleted1[1:10,1:10]
dim(trainuserscompleted1) # 422 508
outcome = as.data.frame(trainuserscompleted1)
#outcome = outcome[,-1]

Top1_Hotel = integer(nrow(outcome))
Top2_Hotel = integer(nrow(outcome))
Top3_Hotel = integer(nrow(outcome))
Top4_Hotel = integer(nrow(outcome))
Top5_Hotel = integer(nrow(outcome))

for (i in 1:nrow(outcome)) {
  a = as.matrix(outcome[i,])[1,]
  Top1_Hotel[i] = names(a[order(a,decreasing=TRUE)[1]])
  Top2_Hotel[i] = names(a[order(a,decreasing=TRUE)[2]])
  Top3_Hotel[i] = names(a[order(a,decreasing=TRUE)[3]])
  Top4_Hotel[i] = names(a[order(a,decreasing=TRUE)[4]])
  Top5_Hotel[i] = names(a[order(a,decreasing=TRUE)[5]])
}

df1 <- data.frame(Top1_Hotel, Top2_Hotel, Top3_Hotel, Top4_Hotel, Top5_Hotel, stringsAsFactors = TRUE)
rownames(df1) = rownames(users)
write.csv(df1, file = "Recommended Hotel For each user using ALS.csv")
abserrs = apply(testevents, 1, function(row) abs(trainuserscompleted1[row[1],row[2]] - users[row[1],row[2]])) # row[1] ~ user, row[2] ~ item
mean(t(abserrs), na.rm=TRUE) # show the MAE
```

### compute the MAE for the predictions made for the test events fir model 2 - fit2 (Using SVD)
```{r}
rownames(trainuserscompleted2) = rownames(users) # copy across the user names
colnames(trainuserscompleted2) = colnames(users) # copy across the item names
# Output recommendation using ALS.
trainuserscompleted2[1:10,1:10]
dim(trainuserscompleted2) # 422 508
outcome = as.data.frame(trainuserscompleted2)
#outcome = outcome[,-1]

Top1_Hotel = integer(nrow(outcome))
Top2_Hotel = integer(nrow(outcome))
Top3_Hotel = integer(nrow(outcome))
Top4_Hotel = integer(nrow(outcome))
Top5_Hotel = integer(nrow(outcome))

for (i in 1:nrow(outcome)) {
  a = as.matrix(outcome[i,])[1,]
  Top1_Hotel[i] = names(a[order(a,decreasing=TRUE)[1]])
  Top2_Hotel[i] = names(a[order(a,decreasing=TRUE)[2]])
  Top3_Hotel[i] = names(a[order(a,decreasing=TRUE)[3]])
  Top4_Hotel[i] = names(a[order(a,decreasing=TRUE)[4]])
  Top5_Hotel[i] = names(a[order(a,decreasing=TRUE)[5]])
}

df2 <- data.frame(Top1_Hotel, Top2_Hotel, Top3_Hotel, Top4_Hotel, Top5_Hotel, stringsAsFactors = TRUE)
rownames(df2) = rownames(users)
write.csv(df2, file = "Recommended Hotel For each user using SVD.csv")
abserrs = apply(testevents, 1, function(row) abs(trainuserscompleted2[row[1],row[2]] - users[row[1],row[2]])) # row[1] ~ user, row[2] ~ item
mean(t(abserrs), na.rm=TRUE) # show the MAE
```
#################################
# for comparison we can make the predictions for the test set events manually from the fractorised matrices
################################

### add user and item names to U and V so we can index them by user name and item name for ALS
```{r}
rownames(fit1$u) = sort(unique(events$user))
rownames(fit1$v) = sort(unique(events$item))
fit1$u[1:10,1:5]; fit1$v[1:10,1:5] 
```

### add user and item names to U and V so we can index them by user name and item name for SVD
```{r}
rownames(fit2$u) = sort(unique(events$user))
rownames(fit2$v) = sort(unique(events$item))
fit2$u[1:10,1:5]; fit2$v[1:10,1:5] 
```

### compute a predicted rating for each (user,item) in testevents forn ALS
### prediction = sum( userfeatures (from u matrix) * singularvalues (d matrix) * itemfeatures (from v matrix) )
```{r}
prats1 = apply(testevents,1,function(row) c(sum(fit1$u[row[1],] * fit1$d * fit1$v[row[2],]), row[3])) # row[1] ~ user, row[2] ~ item
head(t(prats1))
length(prats1)
df = data.frame(prats1)
dim(df)
df = t(df)
str(testevents)
testevents$prediction = df[,1]
testevents$MAE = abs(testevents$rating - as.numeric(testevents$prediction))
sum(testevents$MAE)/925
write.csv(testevents,"testevents_ALS.csv")
```

### compute a predicted rating for each (user,item) in testevents forn SVD
### prediction = sum( userfeatures (from u matrix) * singularvalues (d matrix) * itemfeatures (from v matrix) )
```{r}
prats2 = apply(testevents,1,function(row) c(sum(fit2$u[row[1],] * fit2$d * fit2$v[row[2],]), row[3])) # row[1] ~ user, row[2] ~ item
head(t(prats2))
length(prats2)
df = data.frame(prats2)
dim(df)
df = t(df)
str(testevents)
testevents$prediction = df[,1]
testevents$MAE = abs(testevents$rating - as.numeric(testevents$prediction))
sum(testevents$MAE)/925
write.csv(testevents,"testevents_SVD.csv")
```

### Average Mean Absolute error for ALS ans SVD along with its confusion Matrix
```{r}
cat("Average Mean Absolute error and Confusion Matrix Using ALS \n\n")
preds = as.numeric(unlist(prats1))
cat("avg MAE =",avgMAE(preds))
cat("Confusion Matrix with Threshold Like as 3 \n \n")
showCM(preds, like=3)
cat("Confusion Matrix with Threshold Like as 2 \n \n \n")
showCM(preds, like=2)

cat("\n \nAverage Mean Absolute error and Confusion Matrix Using SVD \n\n")
preds = as.numeric(unlist(prats2))
cat("avg MAE =",avgMAE(preds))
cat("Confusion Matrix with Threshold Like as 3 \n \n")
showCM(preds, like=3)
cat("Confusion Matrix with Threshold Like as 2 \n \n")
showCM(preds, like=2)
```